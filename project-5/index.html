<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<title>Project 5: Fun with Diffusion Models</title>
<style>
body {
font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
margin: 0;
padding: 2rem;
background: #fafafa;
color: #222;
}
h1, h2, h3 {
margin-top: 2rem;
margin-bottom: 0.5rem;
}
h1 {
text-align: center;
}
section {
margin-bottom: 2rem;
padding: 1.5rem;
background: #fff;
border-radius: 8px;
box-shadow: 0 1px 3px rgba(0,0,0,0.06);
}
.subsection {
margin-top: 1rem;
margin-bottom: 1rem;
padding-top: 0.5rem;
border-top: 1px solid #eee;
}
figure {
margin: 1rem 0;
display: inline-block;
text-align: center;
}
figure img {
max-width: 280px;
border-radius: 6px;
border: 1px solid #ddd;
}
figure figcaption {
font-size: 0.9rem;
color: #555;
margin-top: 0.35rem;
}
.image-row {
display: flex;
flex-wrap: wrap;
gap: 1rem;
}
pre {
background: #111;
color: #f1f1f1;
padding: 0.75rem 1rem;
border-radius: 6px;
overflow-x: auto;
font-size: 0.9rem;
}
code {
font-family: SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
}
.note {
font-size: 0.9rem;
color: #666;
margin-top: 0.25rem;
}
</style>
</head>
<body>

<h1>Project 5: Fun with Diffusion Models</h1>

<!-- ========================================================= -->
<!-- Part 0: Prompting & Sampling -->
<!-- ========================================================= -->
<section id="part-0">
<h2>Part 0 – Prompting and Sampling</h2>

To demonstrate the usage of the DeepFloyd IF diffusion model, below are a few examples of different prompts using 20 inference steps with stage 1 of the model, which generates images at 64x64 resolution. Using a seed value of 180:

<div class="subsection" id="part-0-images">
<h3>Images generated with num_inference_steps=20</h3>

<div class="image-row">
<figure>
<img src="images/64vs256/01_64px_20.png" alt="01_64px_20.png" />
<figcaption>Prompt 1 – 'a photo of a hipster barista'<br /></figcaption>
</figure>
<figure>
<img src="images/64vs256/02_64px_20.png" alt="02_64px_20.png" />
<figcaption>Prompt 2 – 'a man wearing a hat'<br /></figcaption>
</figure>
<figure>
<img src="images/64vs256/03_64px_20.png" alt="03_64px_20.png" />
<figcaption>Prompt 3 – 'a rocket ship'<br /></figcaption>
</figure>
</div>
</div>
 
Using stage 2, we can take the output of stage 1 and upscale them to 256x256 resolution:

<div class="subsection" id="part-0-images">
<div class="image-row">
<figure>
<img src="images/64vs256/01_256px_20.png" alt="01_256px_20.png" />
<figcaption>Prompt 1 – 'a photo of a hipster barista'<br /></figcaption>
</figure>
<figure>
<img src="images/64vs256/02_256px_20.png" alt="02_256px_20.png" />
<figcaption>Prompt 2 – 'a man wearing a hat'<br /></figcaption>
</figure>
<figure>
<img src="images/64vs256/03_256px_20.png" alt="03_256px_20.png" />
<figcaption>Prompt 3 – 'a rocket ship'<br /></figcaption>
</figure>
</div>
</div>

By increasing the inference steps, we can generate higher quality images at cost of more compute time. Below are the stage 2 outputs with the number of inference steps at 100:

<div class="subsection" id="part-0-images">
<h3>Images generated with num_inference_steps=100</h3>

<div class="image-row">
<figure>
<img src="images/64vs256/01_256px_100.png" alt="01_256px_100.png" />
<figcaption>Prompt 1 – 'a photo of a hipster barista'<br /></figcaption>
</figure>
<figure>
<img src="images/64vs256/02_256px_100.png" alt="02_256px_100.png" />
<figcaption>Prompt 2 – 'a man wearing a hat'<br /></figcaption>
</figure>
<figure>
<img src="images/64vs256/03_256px_100.png" alt="03_256px_100.png" />
<figcaption>Prompt 3 – 'a rocket ship'<br /></figcaption>
</figure>
</div>
</div>
</section>

<!-- ========================================================= -->
<!-- Part 1.1: Forward Process-->
<!-- ========================================================= -->
<section id="part-1-1">
<h2>Part 1.1 – The forward process</h2>

To start, we have the original Campanile image at 64px:
<div class="image-row">
<figure>
<img src="images/campanile.png" alt="campanile.png" />
</figure>
</div>

For the forward function, we can use <code>alphas_cumprod[t]</code> to obtain the noise coefficient at timestamp <code>t</code>, and <code>torch.randn_like</code> to get &epsilon; &isin; [0, 1), allowing us to compute <code>im_noisy</code>. Below are examples of the Campanile at noise timestamps 250, 500, and 750:

<div class="subsection">
<h3>Campanile at Different Noise Levels</h3>
<div class="image-row">
<figure>
<img src="images/250500750/campanile_250noise.png" alt="campanile_250noise.png" />
<figcaption>Campanile at t = 250</figcaption>
</figure>
<figure>
<img src="images/250500750/campanile_500noise.png" alt="campanile_500noise.png" />
<figcaption>Campanile at t = 500</figcaption>
</figure>
<figure>
<img src="images/250500750/campanile_750noise.png" alt="campanile_750noise.png" />
<figcaption>Campanile at t = 750</figcaption>
</figure>
</div>
</div>
</section>

<!-- ========================================================= -->
<!-- Part 1.2: Gaussian Denoising -->
<!-- ========================================================= -->
<section id="part-1-2">
<h2>Part 1.2 – Classical Denoising</h2>

In order to try to revert the image with noise, we can try the classical method for denoising, namely Gaussian filtering. However, with high noise the effect is limited:

<div class="subsection">
<h3>Noisy vs Gaussian-Denoised Campanile</h3>

<h4>t = 250</h4>
<div class="image-row">
<figure>
<img src="images/250500750/campanile_250noise.png" alt="campanile_250noise.png" />
<figcaption>Noisy Campanile (t = 250)</figcaption>
</figure>
<figure>
<img src="images/250500750/campanile_250denoise_gaussian.png" alt="campanile_250denoise_gaussian.png" />
<figcaption>Gaussian denoised (t = 250)</figcaption>
</figure>
</div>

<h4>t = 500</h4>
<div class="image-row">
<figure>
<img src="images/250500750/campanile_500noise.png" alt="campanile_500noise.png" />
<figcaption>Noisy Campanile (t = 500)</figcaption>
</figure>
<figure>
<img src="images/250500750/campanile_500denoise_gaussian.png" alt="campanile_500denoise_gaussian.png" />
<figcaption>Gaussian denoised (t = 500)</figcaption>
</figure>
</div>

<h4>t = 750</h4>
<div class="image-row">
<figure>
<img src="images/250500750/campanile_750noise.png" alt="campanile_750noise.png" />
<figcaption>Noisy Campanile (t = 750)</figcaption>
</figure>
<figure>
<img src="images/250500750/campanile_750denoise_gaussian.png" alt="campanile_750denoise_gaussian.png" />
<figcaption>Gaussian denoised (t = 750)</figcaption>
</figure>
</div>
</div>
</section>

<!-- ========================================================= -->
<!-- Part 1.3: One-Step Denoising -->
<!-- ========================================================= -->
<section id="part-1-3">
<h2>Part 1.3 – Implementing One Step Denoising</h2>

A more effective method is to use a pretrained diffusion model. Using <code>stage_1.unet</code>, we can estimate the amount of noise in the noisy image. With the forward equation, we can solve for <code>x<sub>0</sub></code> (the original image) given the timestamp <code>t</code>:

<div class="subsection">
<pre><code>at_x0 = im_noisy_cpu - (1 - alpha_cumprod).sqrt() * noise_est
original_im = at_x0 / alpha_cumprod.sqrt()</code></pre>
</div>

Below are a comparison the original, noisy, and the estimate of the original image for <code>t</code> &isin; [250, 500, 750]:

<div class="subsection">
<h3>Original, Noisy, One-Step Estimate (t = 250, 500, 750)</h3>

<h4>t = 250</h4>
<div class="image-row">
<figure>
<img src="images/campanile.png" alt="campanile.png" />
<figcaption>Original Campanile</figcaption>
</figure>
<figure>
<img src="images/250500750/campanile_250noise.png" alt="campanile_250noise.png" />
<figcaption>Noisy (t = 250)</figcaption>
</figure>
<figure>
<img src="images/250500750/campanile_250denoise_onestep.png" alt="campanile_250denoise_onestep.png" />
<figcaption>One-step estimate of original (t = 250)</figcaption>
</figure>
</div>

<h4>t = 500</h4>
<div class="image-row">
<figure>
<img src="images/campanile.png" alt="campanile.png" />
<figcaption>Original Campanile</figcaption>
</figure>
<figure>
<img src="images/250500750/campanile_500noise.png" alt="campanile_500noise.png" />
<figcaption>Noisy (t = 500)</figcaption>
</figure>
<figure>
<img src="images/250500750/campanile_500denoise_onestep.png" alt="campanile_500denoise_onestep.png" />
<figcaption>One-step estimate of original (t = 500)</figcaption>
</figure>
</div>

<h4>t = 750</h4>
<div class="image-row">
<figure>
<img src="images/campanile.png" alt="campanile.png" />
<figcaption>Original Campanile</figcaption>
</figure>
<figure>
<img src="images/250500750/campanile_750noise.png" alt="campanile_750noise.png" />
<figcaption>Noisy (t = 750)</figcaption>
</figure>
<figure>
<img src="images/250500750/campanile_750denoise_onestep.png" alt="campanile_750denoise_onestep.png" />
<figcaption>One-step estimate of original (t = 750)</figcaption>
</figure>
</div>
</div>
</section>

<!-- ========================================================= -->
<!-- Part 1.4: Iterative Denoising-->
<!-- ========================================================= -->
<section id="part-1-4">
<h2>Part 1.4 – Iterative Denoising</h2>

Instead of using one step, we can obtain better results by iterativly denoising from step <code>t</code> until step 0. However, this means running the diffusion model 1000 times in the worst case, which is slow and costly. Fortunately, we can speed up the computation by first defining series of strided timestamps, starting at close to 1000 and ending at 0. For the examples below, we will use <code>strided_timestamps = [990, 960, ..., 30, 0]</code>. Then, we can use the formula

<div class="image-row">
<figure>
<img src="images/equation.png" alt="equation.png" />
</figure>
</div>

to compute <code>x</code> at timestamp <code>T</code>, where <code>T</code> (or <code>prev_t</code>) is the next timestamp after the current timestamp <code>t</code> in <code>strided_timestamps</code>. First, we compute the constants:

<div class="subsection">
<pre><code>alpha_cumprod = alphas_cumprod[t]
alpha_cumprod_prev = alphas_cumprod[prev_t]
alpha_t = alpha_cumprod / alpha_cumprod_prev
beta_t = 1 - alpha_t</code></pre>
</div>

Then, we can compute <code>x<sub>T</sub></code> by using the one-step estimate of <code>x<sub>0</sub></code> as follows:

<div class="subsection">
<pre><code>x_0 = (image - (1 - alpha_cumprod).sqrt() * noise_est) / alpha_cumprod.sqrt()
term_1 = alpha_cumprod_prev.sqrt() * beta_t
term_2 = alpha_t.sqrt() * (1 - alpha_cumprod_prev)
pred_pi_nonoise = (term_1 * x_0 + term_2 * image) / (1 - alpha_cumprod)
pred_prev_image = add_variance(predicted_variance, t, pred_pi_nonoise)</code></pre></div>

Below are some visualizations for the iterative denoising process:

<div class="subsection">
<h3>Denoising Loop Visualizations (i_start = 10)</h3>

<p>Noisy Campanile at t = timestep[10]:</p>
<figure>
<img src="images/ddpm/DDPM_0x5.png" alt="DDPM_0x5.png" />
<figcaption>Initial noisy Campanile (i_start = 10)</figcaption>
</figure>

<h4>The iterative denoising loop</h4>
<div class="image-row">
<figure>
<img src="images/ddpm/DDPM_0x5.png" alt="DDPM_0x5.png" />
<figcaption>Step 0 (initial)</figcaption>
</figure>
<figure>
<img src="images/ddpm/DDPM_1x5.png" alt="DDPM_1x5.png" />
<figcaption>Step 5</figcaption>
</figure>
<figure>
<img src="images/ddpm/DDPM_2x5.png" alt="DDPM_2x5.png" />
<figcaption>Step 10</figcaption>
</figure>
<figure>
<img src="images/ddpm/DDPM_3x5.png" alt="DDPM_3x5.png" />
<figcaption>Step 15</figcaption>
</figure>
<figure>
<img src="images/ddpm/DDPM_4x5.png" alt="DDPM_4x5.png" />
<figcaption>Step 20</figcaption>
</figure>
<figure>
<img src="images/ddpm/final_ddpm.png" alt="final_ddpm.png" />
<figcaption>Step 23 (final)</figcaption>
</figure>
</div>

<h4>Final predicted clean images</h4>
<div class="image-row">
<figure>
<img src="images/ddpm/final_ddpm.png" alt="final_ddpm.png" />
<figcaption>Iterative denoising</figcaption>
</figure>
<figure>
<img src="images/ddpm/final_onestep.png" alt="final_onestep.png" />
<figcaption>One-step denoising (from part 1.3)</figcaption>
</figure>
<figure>
<img src="images/ddpm/final_gaussianblur.png" alt="final_gaussianblur.png" />
<figcaption>Gaussian blur baseline (from part 1.2)</figcaption>
</figure>
</div>
</div>
</section>

<!-- ========================================================= -->
<!-- Part 1.5: Unconditional Sampling -->
<!-- ========================================================= -->
<section id="part-1-5">
<h2>Part 1.5 – Diffusion Model Sampling</h2>

Starting with pure noise, we can obtain random denoise images by setting the starting index of <code>strided_timestamps</code> to 0, and using the prompt <code>'a high quality photo'</code>. Below are a few examples:

<div class="subsection">
<div class="image-row">
<figure>
<img src="images/ahqi/ahqi_1.png" alt="ahqi_1.png" />
<figcaption>Sample 1</figcaption>
</figure>
<figure>
<img src="images/ahqi/ahqi_2.png" alt="ahqi_2.png" />
<figcaption>Sample 2</figcaption>
</figure>
<figure>
<img src="images/ahqi/ahqi_3.png" alt="ahqi_3.png" />
<figcaption>Sample 3</figcaption>
</figure>
<figure>
<img src="images/ahqi/ahqi_4.png" alt="ahqi_4.png" />
<figcaption>Sample 4</figcaption>
</figure>
<figure>
<img src="images/ahqi/ahqi_5.png" alt="ahqi_5.png" />
<figcaption>Sample 5</figcaption>
</figure>
</div>
</div>
</section>

<!-- ========================================================= -->
<!-- Part 1.6: CFG Sampling -->
<!-- ========================================================= -->
<section id="part-1-6">
<h2>Part 1.6 – Classifier-Free Guidance (CFG)</h2>

To improve the quality of the images, we can compute both a noise estiamte conditioned on the text prompt, and the unconditional noise estimate, based on the null prompt <code>''</code>. Denoting the conditional noise estimate as &epsilon;<sub>c</sub> and the unconditional noise estimate as &epsilon;<sub>u</sub>, we let our noise estimate be &epsilon; = &epsilon;<sub>u</sub> + &gamma;(&epsilon;<sub>c</sub> - &epsilon;<sub>u</sub>). Note that we have &epsilon; = &epsilon;<sub>u</sub> and &epsilon; = &epsilon;<sub>c</sub> for &gamma; = 0 and &gamma; = 1 respectively. However, when &gamma; > 1, we can get much higher equality images for reasons still dicussed today. This technique is known as <strong>classifier-free guidance</strong>, and we can implement the noise estimate as follows:

<div class="subsection">
<pre><code>noise_est_cfg = uncond_noise_est + scale * (noise_est - uncond_noise_est)
</code></pre>
</div>

By setting <code>scale = 7</code> (&gamma; = 7) and the conditional & unconditional prompts be <code>'a high quality photo'</code> & the null prompt <code>''</code>, we get the following sample images:

<div class="subsection">
<div class="image-row">
<figure>
<img src="images/ahqi/cfg_ahqi_1.png" alt="cfg_ahqi_1.png" />
<figcaption>CFG sample 1</figcaption>
</figure>
<figure>
<img src="images/ahqi/cfg_ahqi_2.png" alt="cfg_ahqi_2.png" />
<figcaption>CFG sample 2</figcaption>
</figure>
<figure>
<img src="images/ahqi/cfg_ahqi_3.png" alt="cfg_ahqi_3.png" />
<figcaption>CFG sample 3</figcaption>
</figure>
<figure>
<img src="images/ahqi/cfg_ahqi_4.png" alt="cfg_ahqi_4.png" />
<figcaption>CFG sample 4</figcaption>
</figure>
<figure>
<img src="images/ahqi/cfg_ahqi_5.png" alt="cfg_ahqi_5.png" />
<figcaption>CFG sample 5</figcaption>
</figure>
</div>
</div>
</section>

<!-- ========================================================= -->
<!-- Part 1.7: SDEdit-style Edits & Inpainting-->
<!-- ========================================================= -->
<section id="part-1-7">
<h2>Part 1.7 – Image-to-image Translation</h2>

Similar to how we added noise to an existing image before denoising the result in part 1.4, we can use the <code>iterative_denoise_cfg</code> function to get a result that is of higher quality, as opposed to merely a prediction of the original. By adjusting the starting amount of noise to the Campanile with the timestamp index <code>i_start</code>, where a higher index means less noise, we get a series of edits to the that gradually go from entirely new to resembling the original image:

<!-- 1.7: Edits of Campanile -->
<div class="subsection">
<h3>Edits of the Campanile (Noise Levels [1, 3, 5, 7, 10, 20])</h3>
<div class="image-row">
<figure>
<img src="images/campanile.png" alt="campanile.png" />
<figcaption>Original Campanile</figcaption>
</figure>
<figure>
<img src="images/noise/campanile_noise1.png" alt="campanile_noise1.png" />
<figcaption>i_start = 1</figcaption>
</figure>
<figure>
<img src="images/noise/campanile_noise3.png" alt="campanile_noise3.png" />
<figcaption>i_start = 3</figcaption>
</figure>
<figure>
<img src="images/noise/campanile_noise5.png" alt="campanile_noise5.png" />
<figcaption>i_start = 5</figcaption>
</figure>
<figure>
<img src="images/noise/campanile_noise7.png" alt="campanile_noise7.png" />
<figcaption>i_start = 7</figcaption>
</figure>
<figure>
<img src="images/noise/campanile_noise10.png" alt="campanile_noise10.png" />
<figcaption>i_start = 10</figcaption>
</figure>
<figure>
<img src="images/noise/campanile_noise20.png" alt="campanile_noise20.png" />
<figcaption>i_start = 20</figcaption>
</figure>
</div>
</div>

Below are 2 other examples of editing similar images:

<!-- 1.7: Edits of own test images -->
<div class="subsection">
<h4>Edits of Eiffel Tower</h4>
<div class="image-row">
<figure>
<img src="images/noise/eiffel.png" alt="eiffel.png" />
<figcaption>Original Campanile</figcaption>
</figure>
<figure>
<img src="images/noise/eiffel_noise1.png" alt="eiffel_noise1.png" />
<figcaption>i_start = 1</figcaption>
</figure>
<figure>
<img src="images/noise/eiffel_noise3.png" alt="eiffel_noise3.png" />
<figcaption>i_start = 3</figcaption>
</figure>
<figure>
<img src="images/noise/eiffel_noise5.png" alt="eiffel_noise5.png" />
<figcaption>i_start = 5</figcaption>
</figure>
<figure>
<img src="images/noise/eiffel_noise7.png" alt="eiffel_noise7.png" />
<figcaption>i_start = 7</figcaption>
</figure>
<figure>
<img src="images/noise/eiffel_noise10.png" alt="eiffel_noise10.png" />
<figcaption>i_start = 10</figcaption>
</figure>
<figure>
<img src="images/noise/eiffel_noise20.png" alt="eiffel_noise20.png" />
<figcaption>i_start = 20</figcaption>
</figure>
</div>

<h4>Edits of Sydney Opera House</h4>
<div class="image-row">
<figure>
<img src="images/part1_7_own2_original.png" alt="Own test image 2 original" />
<figcaption>Original image 2</figcaption>
</figure>
<figure>
<img src="images/part1_7_own2_edit_level1.png" alt="Own image 2 edit level 1" />
<figcaption>Noise index 1</figcaption>
</figure>
<!-- add 3, 5, 7, 10, 20 -->
</div>
</div>

<!-- 1.7: Web image edits -->
<div class="subsection">
<h3>1.7.1 – Editing Hand-Drawn and Web Images</h3>

The same procedure can also be done for images that are hand-drawn or nonrealistic:

<div class="image-row">
<figure>
<img src="images/part1_7_web_original.png" alt="Web image original" />
<figcaption>Original web image</figcaption>
</figure>
<figure>
<img src="images/part1_7_web_edit_level1.png" alt="Web image edit level 1" />
<figcaption>Noise index 1</figcaption>
</figure>
<!-- add 3, 5, 7, 10, 20 (+ extras if any) -->
</div>
</div>

<!-- 1.7: Hand-drawn image edits -->
<div class="subsection">
<h3>1.7.4 – Edits of 2 Hand-Drawn Images</h3>

<div class="image-row">
<figure>
<img src="images/part1_7_hand1_edit_level1.png" alt="Hand-drawn 1 edit level 1" />
<figcaption>Noise index 1</figcaption>
</figure>
<!-- add 3, 5, 7, 10, 20 -->
</div>

<h4>Hand Drawn Image 2</h4>
<div class="image-row">
<figure>
<img src="images/part1_7_hand2_original.png" alt="Hand-drawn 2 original" />
<figcaption>Original hand-drawn image 2</figcaption>
</figure>
<figure>
<img src="images/part1_7_hand2_edit_level1.png" alt="Hand-drawn 2 edit level 1" />
<figcaption>Noise index 1</figcaption>
</figure>
<!-- add 3, 5, 7, 10, 20 -->
</div>
</div>

<!-- 1.7: Inpainting -->
<div class="subsection">
<h3>1.7.5 – Inpainting</h3>

<h4>Code: inpaint function</h4>
<pre><code># TODO
# def inpaint(original_image, mask, prompt_embeds, uncond_prompt_embeds, timesteps, scale=7, ...):
# ...
# return inpainted_image</code></pre>

<h4>Campanile Inpainting</h4>
<div class="image-row">
<figure>
<img src="images/part1_7_campanile_inpaint_mask.png" alt="Campanile inpaint mask" />
<figcaption>Campanile with inpaint mask overlay</figcaption>
</figure>
<figure>
<img src="images/part1_7_campanile_inpaint_result.png" alt="Campanile inpaint result" />
<figcaption>Inpainted Campanile</figcaption>
</figure>
</div>

<h4>Inpainting on 2 of Your Own Images</h4>
<h5>Own Image 1</h5>
<div class="image-row">
<figure>
<img src="images/part1_7_own1_inpaint_mask.png" alt="Own image 1 inpaint mask" />
<figcaption>Own image 1 – mask</figcaption>
</figure>
<figure>
<img src="images/part1_7_own1_inpaint_result.png" alt="Own image 1 inpaint result" />
<figcaption>Own image 1 – inpainted</figcaption>
</figure>
</div>

<h5>Own Image 2</h5>
<div class="image-row">
<figure>
<img src="images/part1_7_own2_inpaint_mask.png" alt="Own image 2 inpaint mask" />
<figcaption>Own image 2 – mask</figcaption>
</figure>
<figure>
<img src="images/part1_7_own2_inpaint_result.png" alt="Own image 2 inpaint result" />
<figcaption>Own image 2 – inpainted</figcaption>
</figure>
</div>
</div>

</section>

</body>
</html>
