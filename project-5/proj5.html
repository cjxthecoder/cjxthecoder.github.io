<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<title>Project 5: Fun with Diffusion Models</title>
<style>
body {
font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
margin: 0;
padding: 2rem;
background: #fafafa;
color: #222;
}
h1, h2, h3 {
margin-top: 2rem;
margin-bottom: 0.5rem;
}
h1 {
text-align: center;
}
section {
margin-bottom: 2rem;
padding: 1.5rem;
background: #fff;
border-radius: 8px;
box-shadow: 0 1px 3px rgba(0,0,0,0.06);
}
.subsection {
margin-top: 1rem;
margin-bottom: 1rem;
padding-top: 0.5rem;
border-top: 1px solid #eee;
}
figure {
margin: 1rem 0;
display: inline-block;
text-align: center;
}
figure img {
max-width: 280px;
border-radius: 6px;
border: 1px solid #ddd;
}
figure figcaption {
font-size: 0.9rem;
color: #555;
margin-top: 0.35rem;
}
.image-row {
display: flex;
flex-wrap: wrap;
gap: 1rem;
}
pre {
background: #111;
color: #f1f1f1;
padding: 0.75rem 1rem;
border-radius: 6px;
overflow-x: auto;
font-size: 0.9rem;
}
code {
font-family: SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
}
.note {
font-size: 0.9rem;
color: #666;
margin-top: 0.25rem;
}
</style>
</head>
<body>

<h1>Project 5: Fun with Diffusion Models</h1>
<div align="center">
<p>
by Daniel Cheng | December 13, 2025
</p>
</div>

<!-- ========================================================= -->
<!-- Part 0: Prompting & Sampling -->
<!-- ========================================================= -->
<section id="part-0">
<h2>Part 0 – Prompting and Sampling</h2>

To demonstrate the usage of the DeepFloyd IF diffusion model, below are a few examples of different prompts using 20 inference steps with stage 1 of the model, which generates images at 64x64 resolution. Using a seed value of 180:

<div class="subsection" id="part-0-images">
<h3>Images generated with num_inference_steps=20</h3>

<div class="image-row">
<figure>
<img src="images/64vs256/01_64px_20.png" alt="01_64px_20.png" />
<figcaption>Prompt 1 – 'a photo of a hipster barista'<br /></figcaption>
</figure>
<figure>
<img src="images/64vs256/02_64px_20.png" alt="02_64px_20.png" />
<figcaption>Prompt 2 – 'a man wearing a hat'<br /></figcaption>
</figure>
<figure>
<img src="images/64vs256/03_64px_20.png" alt="03_64px_20.png" />
<figcaption>Prompt 3 – 'a rocket ship'<br /></figcaption>
</figure>
</div>
</div>
 
Using stage 2, we can take the output of stage 1 and upscale it to 256x256 resolution:

<div class="subsection" id="part-0-images">
<div class="image-row">
<figure>
<img src="images/64vs256/01_256px_20.png" alt="01_256px_20.png" />
<figcaption>Prompt 1 – 'a photo of a hipster barista'<br /></figcaption>
</figure>
<figure>
<img src="images/64vs256/02_256px_20.png" alt="02_256px_20.png" />
<figcaption>Prompt 2 – 'a man wearing a hat'<br /></figcaption>
</figure>
<figure>
<img src="images/64vs256/03_256px_20.png" alt="03_256px_20.png" />
<figcaption>Prompt 3 – 'a rocket ship'<br /></figcaption>
</figure>
</div>
</div>

By increasing the inference steps, we can generate higher quality images at the cost of more compute time. Below are the stage 2 outputs with the number of inference steps at 100:

<div class="subsection" id="part-0-images">
<h3>Images generated with num_inference_steps=100</h3>

<div class="image-row">
<figure>
<img src="images/64vs256/01_256px_100.png" alt="01_256px_100.png" />
<figcaption>Prompt 1 – 'a photo of a hipster barista'<br /></figcaption>
</figure>
<figure>
<img src="images/64vs256/02_256px_100.png" alt="02_256px_100.png" />
<figcaption>Prompt 2 – 'a man wearing a hat'<br /></figcaption>
</figure>
<figure>
<img src="images/64vs256/03_256px_100.png" alt="03_256px_100.png" />
<figcaption>Prompt 3 – 'a rocket ship'<br /></figcaption>
</figure>
</div>
</div>

From the above, we can see that with longer prompts, we can generate images with more specific details.
</section>

<!-- ========================================================= -->
<!-- Part 1.1: Forward Process-->
<!-- ========================================================= -->
<section id="part-1-1">
<h2>Part 1.1 – The forward process</h2>

To start, we have the original Campanile image at 64px:
<div align="center">
<figure>
<img src="images/original/campanile.png" alt="campanile.png" />
</figure>
</div>

To add noise to an image <code>x<sub>0</sub></code>, we can use the forward process and compute
<div align="center">
<figure>
<img src="images/original/forward.png" alt="forward.png" />
</figure>
</div>

for a given timestamp <code>t</code> &isin; [0, 1, ..., 999, 1000]. The noise coefficient at timestamp <code>t</code> can be obtained using

<pre><code>alphas_cumprod = stage_1.scheduler.alphas_cumprod
alpha_cumprod_t = alphas_cumprod[t]
</code></pre>

for a given <code>t</code>. Below are examples of the Campanile at noise timestamps 250, 500, and 750:

<div class="subsection">
<h3>Campanile at Different Noise Levels</h3>
<div class="image-row">
<figure>
<img src="images/250500750/campanile_250noise.png" alt="campanile_250noise.png" />
<figcaption>Campanile at t = 250</figcaption>
</figure>
<figure>
<img src="images/250500750/campanile_500noise.png" alt="campanile_500noise.png" />
<figcaption>Campanile at t = 500</figcaption>
</figure>
<figure>
<img src="images/250500750/campanile_750noise.png" alt="campanile_750noise.png" />
<figcaption>Campanile at t = 750</figcaption>
</figure>
</div>
</div>
</section>

<!-- ========================================================= -->
<!-- Part 1.2: Gaussian Denoising -->
<!-- ========================================================= -->
<section id="part-1-2">
<h2>Part 1.2 – Classical Denoising</h2>

In order to try to revert the image with noise, we can attempt the classical method for denoising. Namely, Gaussian filtering. However, with high noise, the effect is limited:

<div class="subsection">
<h3>Noisy vs Gaussian-Denoised Campanile</h3>

<div class="image-row">
<figure>
<img src="images/250500750/campanile_250noise.png" alt="campanile_250noise.png" />
<figcaption>Noisy Campanile (t = 250)</figcaption>
</figure>
<figure>
<img src="images/250500750/campanile_250denoise_gaussian.png" alt="campanile_250denoise_gaussian.png" />
<figcaption>5&times;5 Gaussian denoised</figcaption>
</figure>
</div>

<div class="image-row">
<figure>
<img src="images/250500750/campanile_500noise.png" alt="campanile_500noise.png" />
<figcaption>Noisy Campanile (t = 500)</figcaption>
</figure>
<figure>
<img src="images/250500750/campanile_500denoise_gaussian.png" alt="campanile_500denoise_gaussian.png" />
<figcaption>5&times;5 Gaussian denoised</figcaption>
</figure>
</div>

<div class="image-row">
<figure>
<img src="images/250500750/campanile_750noise.png" alt="campanile_750noise.png" />
<figcaption>Noisy Campanile (t = 750)</figcaption>
</figure>
<figure>
<img src="images/250500750/campanile_750denoise_gaussian.png" alt="campanile_750denoise_gaussian.png" />
<figcaption>5&times;5 Gaussian denoised</figcaption>
</figure>
</div>
</div>
</section>

<!-- ========================================================= -->
<!-- Part 1.3: One-Step Denoising -->
<!-- ========================================================= -->
<section id="part-1-3">
<h2>Part 1.3 – Implementing One Step Denoising</h2>

A more effective method is to use a pretrained diffusion model. Using <code>stage_1.unet</code>, we can estimate the amount of noise in the noisy image. With the forward process defined above, we can solve for <code>x<sub>0</sub></code> (the original image) given <code>x<sub>t</sub></code> (the noisy image) at timestamp <code>t</code>:

<pre><code>at_x0 = im_noisy_cpu - (1 - alpha_cumprod).sqrt() * noise_est
original_im = at_x0 / alpha_cumprod.sqrt()</code></pre>

This method of estimating the clean image given a noisy image is known as <strong>one-step denoising</strong>. Below is a comparison of the original, noisy, and the estimate of the original image for <code>t</code> &isin; [250, 500, 750]:

<div class="subsection">
<h3>Original, Noisy, One-Step Estimate (t = 250, 500, 750)</h3>

<h4>t = 250</h4>
<div class="image-row">
<figure>
<img src="images/original/campanile.png" alt="campanile.png" />
<figcaption>Original Campanile</figcaption>
</figure>
<figure>
<img src="images/250500750/campanile_250noise.png" alt="campanile_250noise.png" />
<figcaption>Noisy Campanile (t = 250)</figcaption>
</figure>
<figure>
<img src="images/250500750/campanile_250denoise_onestep.png" alt="campanile_250denoise_onestep.png" />
<figcaption>One-step estimate of original (t = 250)</figcaption>
</figure>
</div>

<h4>t = 500</h4>
<div class="image-row">
<figure>
<img src="images/original/campanile.png" alt="campanile.png" />
<figcaption>Original Campanile</figcaption>
</figure>
<figure>
<img src="images/250500750/campanile_500noise.png" alt="campanile_500noise.png" />
<figcaption>Noisy Campanile (t = 500)</figcaption>
</figure>
<figure>
<img src="images/250500750/campanile_500denoise_onestep.png" alt="campanile_500denoise_onestep.png" />
<figcaption>One-step estimate of original (t = 500)</figcaption>
</figure>
</div>

<h4>t = 750</h4>
<div class="image-row">
<figure>
<img src="images/original/campanile.png" alt="campanile.png" />
<figcaption>Original Campanile</figcaption>
</figure>
<figure>
<img src="images/250500750/campanile_750noise.png" alt="campanile_750noise.png" />
<figcaption>Noisy Campanile (t = 750)</figcaption>
</figure>
<figure>
<img src="images/250500750/campanile_750denoise_onestep.png" alt="campanile_750denoise_onestep.png" />
<figcaption>One-step estimate of original (t = 750)</figcaption>
</figure>
</div>
</div>
</section>

<!-- ========================================================= -->
<!-- Part 1.4: Iterative Denoising-->
<!-- ========================================================= -->
<section id="part-1-4">
<h2>Part 1.4 – Iterative Denoising</h2>

Instead of using one-step, we can obtain better results by iteratively denoising from step <code>t</code> until step 0. However, this means running the diffusion model 1000 times in the worst case, which is slow and costly. Fortunately, we can speed up the computation by first defining a series of strided timestamps, starting at close to 1000 and ending at 0. For the examples below, we will use <code>strided_timestamps = [990, 960, ..., 30, 0]</code>. Then, we can use the formula

<div align="center">
<figure>
<img src="images/original/equation.png" alt="equation.png" />
</figure>
</div>

to compute <code>x</code> at timestamp <code>T</code>, where <code>T</code> (or <code>prev_t</code>) is the next timestamp after the current timestamp in the strided timestamps. First, we use the definition of the constants, where <code>alpha_cumprod_t</code> is the variable with the bar:

<pre><code>alpha_cumprod_t = alphas_cumprod[t]
alpha_cumprod_prev = alphas_cumprod[prev_t]
alpha_t = alpha_cumprod_t / alpha_cumprod_prev
beta_t = 1 - alpha_t</code></pre>

Then, we can get an approximation of <code>x<sub>0</sub></code> by using the one-step estimate. The estimated variance will be computed along with the noise estimate, allowing us to calculate <code>x<sub>T</sub></code> using the formula above and obtain the image estimate for the next step. Below are some visualizations for the iterative denoising process:

<div class="subsection">
<h3>Denoising Loop Visualizations (i_start = 10)</h3>

<h4>Noisy Campanile at <code>t = strided_timestamps[i_start]</code> (t = 690):</h4>
<figure>
<img src="images/ddpm/DDPM_0x5.png" alt="DDPM_0x5.png" />
<figcaption>Initial noisy Campanile (i_start = 10)</figcaption>
</figure>

<h4>The iterative denoising loop</h4>
<div class="image-row">
<figure>
<img src="images/ddpm/DDPM_0x5.png" alt="DDPM_0x5.png" />
<figcaption>Step 0 (initial)</figcaption>
</figure>
<figure>
<img src="images/ddpm/DDPM_1x5.png" alt="DDPM_1x5.png" />
<figcaption>Step 5</figcaption>
</figure>
<figure>
<img src="images/ddpm/DDPM_2x5.png" alt="DDPM_2x5.png" />
<figcaption>Step 10</figcaption>
</figure>
<figure>
<img src="images/ddpm/DDPM_3x5.png" alt="DDPM_3x5.png" />
<figcaption>Step 15</figcaption>
</figure>
<figure>
<img src="images/ddpm/DDPM_4x5.png" alt="DDPM_4x5.png" />
<figcaption>Step 20</figcaption>
</figure>
<figure>
<img src="images/ddpm/final_ddpm.png" alt="final_ddpm.png" />
<figcaption>Step 23 (final)</figcaption>
</figure>
</div>

<h4>Final predicted clean images</h4>
<div class="image-row">
<figure>
<img src="images/ddpm/final_ddpm.png" alt="final_ddpm.png" />
<figcaption>Iterative denoising</figcaption>
</figure>
<figure>
<img src="images/ddpm/final_onestep.png" alt="final_onestep.png" />
<figcaption>One-step denoising</figcaption>
</figure>
<figure>
<img src="images/ddpm/final_gaussianblur.png" alt="final_gaussianblur.png" />
<figcaption>Gaussian blur baseline</figcaption>
</figure>
</div>
</div>

As we can see, the iterative denoising process produced a more detailed image.
</section>

<!-- ========================================================= -->
<!-- Part 1.5: Unconditional Sampling -->
<!-- ========================================================= -->
<section id="part-1-5">
<h2>Part 1.5 – Diffusion Model Sampling</h2>

Starting with pure noise, we can obtain random denoise images by setting the starting index of <code>strided_timestamps</code> to 0, and using the prompt <code>'a high quality photo'</code>. Below are a few examples:

<div class="subsection">
<div class="image-row">
<figure>
<img src="images/ahqi/ahqi_1.png" alt="ahqi_1.png" />
<figcaption>Sample 1</figcaption>
</figure>
<figure>
<img src="images/ahqi/ahqi_2.png" alt="ahqi_2.png" />
<figcaption>Sample 2</figcaption>
</figure>
<figure>
<img src="images/ahqi/ahqi_3.png" alt="ahqi_3.png" />
<figcaption>Sample 3</figcaption>
</figure>
<figure>
<img src="images/ahqi/ahqi_4.png" alt="ahqi_4.png" />
<figcaption>Sample 4</figcaption>
</figure>
<figure>
<img src="images/ahqi/ahqi_5.png" alt="ahqi_5.png" />
<figcaption>Sample 5</figcaption>
</figure>
</div>
</div>
</section>

<!-- ========================================================= -->
<!-- Part 1.6: CFG Sampling -->
<!-- ========================================================= -->
<section id="part-1-6">
<h2>Part 1.6 – Classifier-Free Guidance (CFG)</h2>

To improve the quality of the images, we can compute both a noise estimate conditioned on the text prompt and the unconditional noise estimate based on the null prompt <code>''</code>. Denoting the conditional noise estimate as &epsilon;<sub>c</sub> and the unconditional noise estimate as &epsilon;<sub>u</sub>, we let our noise estimate be &epsilon; = &epsilon;<sub>u</sub> + &gamma;(&epsilon;<sub>c</sub> - &epsilon;<sub>u</sub>). Note that we have &epsilon; = &epsilon;<sub>u</sub> and &epsilon; = &epsilon;<sub>c</sub> for &gamma; = 0 and &gamma; = 1 respectively. However, when &gamma; > 1, we can get much higher equality images for reasons still discussed today. This technique is known as <strong>classifier-free guidance</strong>. By using &gamma; = 7 and the conditional & unconditional prompts be <code>'a high quality photo'</code> & the null prompt <code>''</code>, we get the following sample images:

<div class="subsection">
<div class="image-row">
<figure>
<img src="images/ahqi/cfg_ahqi_1.png" alt="cfg_ahqi_1.png" />
<figcaption>CFG sample 1</figcaption>
</figure>
<figure>
<img src="images/ahqi/cfg_ahqi_2.png" alt="cfg_ahqi_2.png" />
<figcaption>CFG sample 2</figcaption>
</figure>
<figure>
<img src="images/ahqi/cfg_ahqi_3.png" alt="cfg_ahqi_3.png" />
<figcaption>CFG sample 3</figcaption>
</figure>
<figure>
<img src="images/ahqi/cfg_ahqi_4.png" alt="cfg_ahqi_4.png" />
<figcaption>CFG sample 4</figcaption>
</figure>
<figure>
<img src="images/ahqi/cfg_ahqi_5.png" alt="cfg_ahqi_5.png" />
<figcaption>CFG sample 5</figcaption>
</figure>
</div>
</div>

These settings (&gamma; = 7 and the unconditional prompt being <code>''</code>) will be used in all future usage of the CFG iterative denoise function.
</section>

<!-- ========================================================= -->
<!-- Part 1.7: SDEdit-style Edits & Inpainting-->
<!-- ========================================================= -->
<section id="part-1-7">
<h2>Part 1.7 – Image-to-image Translation</h2>

Similar to how we added noise to an existing image before denoising the result in part 1.4, we can use the <code>iterative_denoise_cfg</code> function to get a result that is of higher quality, as opposed to merely a prediction of the original. By adjusting the starting amount of noise to the Campanile with the timestamp index <code>i_start</code>, where a higher index means less noise, we get a series of edits that gradually go from entirely new to resembling the original image:

<!-- 1.7: Edits of Campanile -->
<div class="subsection">
<h3>Edits of the Campanile (Noise Levels [1, 3, 5, 7, 10, 20])</h3>
<div class="image-row">
<figure>
<img src="images/original/campanile.png" alt="campanile.png" />
<figcaption>Original</figcaption>
</figure>
<figure>
<img src="images/noise/campanile_noise1.png" alt="campanile_noise1.png" />
<figcaption>i_start = 1</figcaption>
</figure>
<figure>
<img src="images/noise/campanile_noise3.png" alt="campanile_noise3.png" />
<figcaption>i_start = 3</figcaption>
</figure>
<figure>
<img src="images/noise/campanile_noise5.png" alt="campanile_noise5.png" />
<figcaption>i_start = 5</figcaption>
</figure>
<figure>
<img src="images/noise/campanile_noise7.png" alt="campanile_noise7.png" />
<figcaption>i_start = 7</figcaption>
</figure>
<figure>
<img src="images/noise/campanile_noise10.png" alt="campanile_noise10.png" />
<figcaption>i_start = 10</figcaption>
</figure>
<figure>
<img src="images/noise/campanile_noise20.png" alt="campanile_noise20.png" />
<figcaption>i_start = 20</figcaption>
</figure>
</div>
</div>

Below are 2 other examples of editing similar images:

<!-- 1.7: Edits of own test images -->
<div class="subsection">
<h4>Test Image 1 (Eiffel Tower)</h4>
<div class="image-row">
<figure>
<img src="images/original/eiffel.png" alt="eiffel.png" />
<figcaption>Original</figcaption>
</figure>
<figure>
<img src="images/noise/eiffel_noise1.png" alt="eiffel_noise1.png" />
<figcaption>i_start = 1</figcaption>
</figure>
<figure>
<img src="images/noise/eiffel_noise3.png" alt="eiffel_noise3.png" />
<figcaption>i_start = 3</figcaption>
</figure>
<figure>
<img src="images/noise/eiffel_noise5.png" alt="eiffel_noise5.png" />
<figcaption>i_start = 5</figcaption>
</figure>
<figure>
<img src="images/noise/eiffel_noise7.png" alt="eiffel_noise7.png" />
<figcaption>i_start = 7</figcaption>
</figure>
<figure>
<img src="images/noise/eiffel_noise10.png" alt="eiffel_noise10.png" />
<figcaption>i_start = 10</figcaption>
</figure>
<figure>
<img src="images/noise/eiffel_noise20.png" alt="eiffel_noise20.png" />
<figcaption>i_start = 20</figcaption>
</figure>
</div>

<h4>Test Image 2 (St. Basil's Cathedral)</h4>
<div class="image-row">
<figure>
<img src="images/original/stbasil.png" alt="stbasil.png" />
<figcaption>Original</figcaption>
</figure>
<figure>
<img src="images/noise/stbasil_noise1.png" alt="stbasil_noise1.png" />
<figcaption>i_start = 1</figcaption>
</figure>
<figure>
<img src="images/noise/stbasil_noise3.png" alt="stbasil_noise3.png" />
<figcaption>i_start = 3</figcaption>
</figure>
<figure>
<img src="images/noise/stbasil_noise5.png" alt="stbasil_noise5.png" />
<figcaption>i_start = 5</figcaption>
</figure>
<figure>
<img src="images/noise/stbasil_noise7.png" alt="stbasil_noise7.png" />
<figcaption>i_start = 7</figcaption>
</figure>
<figure>
<img src="images/noise/stbasil_noise10.png" alt="stbasil_noise10.png" />
<figcaption>i_start = 10</figcaption>
</figure>
<figure>
<img src="images/noise/stbasil_noise20.png" alt="stbasil_noise20.png" />
<figcaption>i_start = 20</figcaption>
</figure>
</div>
</div>

<!-- 1.7.1: Web image edits -->
<div class="subsection">
<h3>1.7.1 – Editing Hand-Drawn and Web Images</h3>

The same procedure can also be done for images that are hand-drawn or non-realistic:

<h4>Web Image (magic gauntlet from <a href="https://gdbrowser.com">gdbrowser.com</a>)</h4>
<div class="image-row">
<figure>
<img src="images/hdr/magic.png" alt="magic.png" />
<figcaption>Original</figcaption>
</figure>
<figure>
<img src="images/hdr/magic_noise1.png" alt="magic_noise1.png" />
<figcaption>i_start = 1</figcaption>
</figure>
<figure>
<img src="images/hdr/magic_noise3.png" alt="magic_noise3.png" />
<figcaption>i_start = 3</figcaption>
</figure>
<figure>
<img src="images/hdr/magic_noise5.png" alt="magic_noise5.png" />
<figcaption>i_start = 5</figcaption>
</figure>
<figure>
<img src="images/hdr/magic_noise7.png" alt="magic_noise7.png" />
<figcaption>i_start = 7</figcaption>
</figure>
<figure>
<img src="images/hdr/magic_noise10.png" alt="magic_noise10.png" />
<figcaption>i_start = 10</figcaption>
</figure>
<figure>
<img src="images/hdr/magic_noise20.png" alt="magic_noise20.png" />
<figcaption>i_start = 20</figcaption>
</figure>
</div>

<h4>Hand Drawn Image 1</h4>
<div class="image-row">
<figure>
<img src="images/hdr/hdr1_original.png" alt="hdr1_original.png" />
<figcaption>Original</figcaption>
</figure>
<figure>
<img src="images/hdr/hdr1_noise1.png" alt="hdr1_noise1.png" />
<figcaption>i_start = 1</figcaption>
</figure>
<figure>
<img src="images/hdr/hdr1_noise3.png" alt="hdr1_noise3.png" />
<figcaption>i_start = 3</figcaption>
</figure>
<figure>
<img src="images/hdr/hdr1_noise5.png" alt="hdr1_noise5.png" />
<figcaption>i_start = 5</figcaption>
</figure>
<figure>
<img src="images/hdr/hdr1_noise7.png" alt="hdr1_noise7.png" />
<figcaption>i_start = 7</figcaption>
</figure>
<figure>
<img src="images/hdr/hdr1_noise10.png" alt="hdr1_noise10.png" />
<figcaption>i_start = 10</figcaption>
</figure>
<figure>
<img src="images/hdr/hdr1_noise20.png" alt="hdr1_noise20.png" />
<figcaption>i_start = 20</figcaption>
</figure>
</div>

<h4>Hand Drawn Image 2</h4>
<div class="image-row">
<figure>
<img src="images/hdr/hdr2_original.png" alt="hdr2_original.png" />
<figcaption>Original</figcaption>
</figure>
<figure>
<img src="images/hdr/hdr2_noise1.png" alt="hdr2_noise1.png" />
<figcaption>i_start = 1</figcaption>
</figure>
<figure>
<img src="images/hdr/hdr2_noise3.png" alt="hdr2_noise3.png" />
<figcaption>i_start = 3</figcaption>
</figure>
<figure>
<img src="images/hdr/hdr2_noise5.png" alt="hdr2_noise5.png" />
<figcaption>i_start = 5</figcaption>
</figure>
<figure>
<img src="images/hdr/hdr2_noise7.png" alt="hdr2_noise7.png" />
<figcaption>i_start = 7</figcaption>
</figure>
<figure>
<img src="images/hdr/hdr2_noise10.png" alt="hdr2_noise10.png" />
<figcaption>i_start = 10</figcaption>
</figure>
<figure>
<img src="images/hdr/hdr2_noise20.png" alt="hdr2_noise20.png" />
<figcaption>i_start = 20</figcaption>
</figure>
</div>
</div>

<!-- 1.7: Inpainting -->
<div class="subsection">
<h3>1.7.2 – Inpainting</h3>

Using the techniques above, we can also modify our <code>iterative_denoise_cfg</code> function to edit certain sections of an image. To do so, we first define a mask the same size as the image that is 1 at the pixels where we want to edit, and 0 otherwise. For each loop of the denoising process, we replace <code>x<sub>t</sub></code> with <strong>m</strong><code>x<sub>t</sub></code> + (1 - <strong>m</strong>)forward(<code>x<sub>0</sub>, t</code>), where <strong>m</strong> is the mask and <code>x<sub>0</sub></code> is the original image.<br>

<br>Once <code>image</code> is replaced by <code>masked_image</code>, we replace all further occurrences of <code>image</code> except for the last instance, as the image at each step still needs to be updated. Finally, we let our starting noise be purely random and start with a timestamp index of 0, so that the patch we want to change can be sufficiently denoised. Below are the results on the Campanile image:

<h4>Campanile Inpainting</h4>
<div class="image-row">
<figure>
<img src="images/original/campanile.png" alt="campanile.png" />
<figcaption>Original</figcaption>
</figure>
<figure>
<img src="images/inpaint/campanile_mask.png" alt="campanile_mask.png" />
<figcaption>Mask</figcaption>
</figure>
<figure>
<img src="images/inpaint/campanile_inpaint.png" alt="campanile_inpaint.png" />
<figcaption>Inpainted</figcaption>
</figure>
</div>
</div>

Below are 2 other examples of inpainting similar images:

<!-- 1.7.2: Inpaintings of own test images -->
<div class="subsection">
<h4>Eiffel Tower Inpainting</h4>
<div class="image-row">
<figure>
<img src="images/original/eiffel.png" alt="eiffel.png" />
<figcaption>Original</figcaption>
</figure>
<figure>
<img src="images/inpaint/eiffel_mask.png" alt="eiffel_mask.png" />
<figcaption>Mask</figcaption>
</figure>
<figure>
<img src="images/inpaint/eiffel_inpaint.png" alt="eiffel_inpaint.png" />
<figcaption>Inpainted</figcaption>
</figure>
</div>

<h4>St. Basil's Cathedral Inpainting</h4>
<div class="image-row">
<figure>
<img src="images/original/stbasil.png" alt="stbasil.png" />
<figcaption>Original</figcaption>
</figure>
<figure>
<img src="images/inpaint/stbasil_mask.png" alt="stbasil_mask.png" />
<figcaption>Mask</figcaption>
</figure>
<figure>
<img src="images/inpaint/stbasil_inpaint.png" alt="stbasil_inpaint.png" />
<figcaption>Inpainted</figcaption>
</figure>
</div>
</div>

<h3>1.7.3 – Text-Conditioned Image-to-image Translation</h3>

Finally, we can also change our conditional prompt, namely <code>'a high quality photo'</code>, to any other. This will give control over what the noise is projected to, resulting in a series of images that look more like the original image, but also similar to the conditional prompt.

<div class="subsection">
<h4>Campanile with prompt <code>'a pencil'</code></h4>
<div class="image-row">
<figure>
<img src="images/original/campanile.png" alt="campanile.png" />
<figcaption>Original</figcaption>
</figure>
<figure>
<img src="images/conditioned/campanile_pencil1.png" alt="campanile_pencil1.png" />
<figcaption>i_start = 1</figcaption>
</figure>
<figure>
<img src="images/conditioned/campanile_pencil3.png" alt="campanile_pencil3.png" />
<figcaption>i_start = 3</figcaption>
</figure>
<figure>
<img src="images/conditioned/campanile_pencil5.png" alt="campanile_pencil5.png" />
<figcaption>i_start = 5</figcaption>
</figure>
<figure>
<img src="images/conditioned/campanile_pencil7.png" alt="campanile_pencil7.png" />
<figcaption>i_start = 7</figcaption>
</figure>
<figure>
<img src="images/conditioned/campanile_pencil10.png" alt="campanile_pencil10.png" />
<figcaption>i_start = 10</figcaption>
</figure>
<figure>
<img src="images/conditioned/campanile_pencil20.png" alt="campanile_pencil20.png" />
<figcaption>i_start = 20</figcaption>
</figure>
</div>

<h4>Eiffel Tower with prompt <code>'a rocket ship'</code></h4>
<div class="image-row">
<figure>
<img src="images/original/eiffel.png" alt="eiffel.png" />
<figcaption>Original</figcaption>
</figure>
<figure>
<img src="images/conditioned/eiffel_rocket1.png" alt="eiffel_rocket1.png" />
<figcaption>i_start = 1</figcaption>
</figure>
<figure>
<img src="images/conditioned/eiffel_rocket3.png" alt="eiffel_rocket3.png" />
<figcaption>i_start = 3</figcaption>
</figure>
<figure>
<img src="images/conditioned/eiffel_rocket5.png" alt="eiffel_rocket5.png" />
<figcaption>i_start = 5</figcaption>
</figure>
<figure>
<img src="images/conditioned/eiffel_rocket7.png" alt="eiffel_rocket7.png" />
<figcaption>i_start = 7</figcaption>
</figure>
<figure>
<img src="images/conditioned/eiffel_rocket10.png" alt="eiffel_rocket10.png" />
<figcaption>i_start = 10</figcaption>
</figure>
<figure>
<img src="images/conditioned/eiffel_rocket20.png" alt="eiffel_rocket20.png" />
<figcaption>i_start = 20</figcaption>
</figure>
</div>

<h4>St. Basil's Cathedral with prompt <code>'an oil painting of a snowy mountain village'</code></h4>
<div class="image-row">
<figure>
<img src="images/original/stbasil.png" alt="stbasil.png" />
<figcaption>Original</figcaption>
</figure>
<figure>
<img src="images/conditioned/stbasil_oil1.png" alt="stbasil_oil1.png" />
<figcaption>i_start = 1</figcaption>
</figure>
<figure>
<img src="images/conditioned/stbasil_oil3.png" alt="stbasil_oil3.png" />
<figcaption>i_start = 3</figcaption>
</figure>
<figure>
<img src="images/conditioned/stbasil_oil5.png" alt="stbasil_oil5.png" />
<figcaption>i_start = 5</figcaption>
</figure>
<figure>
<img src="images/conditioned/stbasil_oil7.png" alt="stbasil_oil7.png" />
<figcaption>i_start = 7</figcaption>
</figure>
<figure>
<img src="images/conditioned/stbasil_oil10.png" alt="stbasil_oil10.png" />
<figcaption>i_start = 10</figcaption>
</figure>
<figure>
<img src="images/conditioned/stbasil_oil20.png" alt="stbasil_oil20.png" />
<figcaption>i_start = 20</figcaption>
</figure>
</div>
</div>
</section>

<!-- ========================================================= -->
<!-- Part 1.8: Visual Anagrams -->
<!-- ========================================================= -->
<section id="part-1-8">
<h2>Part 1.8 – Visual Anagrams</h2>

We now have the necessary tools to generate visual anagrams, or images that look like another different one when flipped/rotated. As an example for a vertical flip anagram, we would start with 2 prompt embeddings <code>p<sub>1</sub></code> and <code>p<sub>2</sub></code>. For <code>p<sub>1</sub></code>, we would compute the noise estimate &epsilon;<sub>1</sub> normally at each step, but for <code>p<sub>2</sub></code>, we flip the image <code>x<sub>t</sub></code> first before computing the noise estimate, then flip back the estimate to obtain &epsilon;<sub>2</sub>, which would be the noise estimate of the flipped image.<br>

<br>Once this is done, we will use the average of &epsilon;<sub>1</sub> and &epsilon;<sub>2</sub> as the final noise estimate for each step. The variance can also be computed similarly, namely v<sub>1</sub> will be computed in the usual way, while v<sub>2</sub> will be the flipped variance estimate of the flipped <code>x<sub>t</sub></code>, and the final variance estimate will (v<sub>1</sub> + v<sub>2</sub>) / 2. Below are a few examples of such an effect, with <code>p<sub>1</sub></code> being the first prompt and <code>p<sub>2</sub></code> being the second:

<div class="subsection">
<h3>Prompts: <code>'an oil painting of an old man'</code> & <code>'an oil painting of people around a campfire'</code></h3>
<div class="image-row">
<figure>
<img src="images/anagram/anagram1_256.png" alt="anagram1_256.png" />
<figcaption>Original</figcaption>
</figure>
<figure>
<img src="images/anagram/anagram1_flip_256.png" alt="anagram1_flip_256.png" />
<figcaption>Flipped</figcaption>
</figure>
</div>

<h3>Prompts: <code>'a lithograph of waterfalls'</code> & <code>'a man wearing a hat'</code></h3>
<div class="image-row">
<figure>
<img src="images/anagram/anagram2_256.png" alt="anagram2_256.png" />
<figcaption>Original</figcaption>
</figure>
<figure>
<img src="images/anagram/anagram2_flip_256.png" alt="anagram2_flip_256.png" />
<figcaption>Flipped</figcaption>
</figure>
</div>

<h3>Prompts: <code>'an oil painting of a snowy mountain village'</code> & <code>'a photo of a dog'</code></h3>
<div class="image-row">
<figure>
<img src="images/anagram/anagram3_256.png" alt="anagram3_256.png" />
<figcaption>Original</figcaption>
</figure>
<figure>
<img src="images/anagram/anagram3_flip_256.png" alt="anagram3_flip_256.png" />
<figcaption>Flipped</figcaption>
</figure>
</div>
</div>
</section>

<!-- ========================================================= -->
<!-- Part 1.9: Hybrid Images -->
<!-- ========================================================= -->
<section id="part-1-9">
<h2>Part 1.9 – Hybrid Images</h2>
 
With the techniques above, we can now also create hybrid images, or images that look like different subjects depending on the viewing distance. The classical way to create a hybrid image is to transform the image you want to see at a far range with a low-pass filter, the image you want to see at close range with a high-pass filter, and combine the 2 transformed images. We can use a similar algorithm in the denoising process, namely by passing the noise estimate from <code>p<sub>1</sub></code> and <code>p<sub>2</sub></code> through a low and high pass filter, respectively.<br>

<br>After doing so, we will add the 2 filtered noises together to get the final noise estimate at each step. This will produce an image that, when viewed close up, shows <code>p<sub>1</sub></code>, but when viewed far away, shows <code>p<sub>2</sub></code>. Unlike the anagram images, we don't need to flip or transform the image to be denoised, as both images should be viewed under the same orientation. Below are several examples:

<div class="subsection">
<div align="center">
<figure>
<img src="images/hybrid/hybrid1_256.png" alt="hybrid1_256.png" />
<figcaption>Prompts: <code>'a lithograph of a skull'</code> (low-pass) & <code>'a lithograph of waterfalls'</code> (high-pass)</figcaption>
</figure>
</div>
<div align="center">
<figure>
<img src="images/hybrid/hybrid2_256.png" alt="hybrid2_256.png" />
<figcaption>Prompts: <code>'a pencil'</code> (low-pass) & <code>'a rocket ship'</code> (high-pass)</figcaption>
</figure>
</div>
<div align="center">
<figure>
<img src="images/hybrid/hybrid3_256.png" alt="hybrid3_256.png" />
<figcaption>Prompts: <code>'a lithograph of waterfalls'</code> (low-pass) & <code>'a photo of a dog'</code> (high-pass)</figcaption>
</figure>
</div>
</div>
</section>

<!-- ========================================================= -->
<!-- Part 2.0: Flow Matching from Scratch -->
<!-- ========================================================= -->
<section id="part-2-1">
<h2>Part 2 – Implementing the UNet from scratch</h2>

Now that we know how we can generate images with the help of a UNet in a denoising model, we will go through implementing one from scratch. More specifically, we will be attempting to generate digits similar to those in the MNIST dataset from pure noise using a denoising UNet that we will create.

<h3>Training an Unconditioned UNet</h3>

The most basic denoiser is a one-step denoiser. Formally, given a noisy image <code>z</code>, we aim to train a denoiser D<sub>&theta;</sub>(z) that can map it to a clean image <code>x</code>. To do this, we can optimize over the L<sup>2</sup> loss E<sub>z,x</sub>||D<sub>&theta;</sub>(z)||<sup>2</sup> while training.<br>

<br>To create a noisy image, we can use the process z = x + &sigma;&epsilon; where &sigma; &isin; [0, 1] and &epsilon; ~ &Nscr;(0, &#119816;). Here, &Nscr; is the standard normal distribution. To visualize the kind of images this process will result in, below is an example of an MNIST digit with progressively more noise as &sigma; gradually increases from 0 to 1:

<div class="image-row">
<figure>
<img src="images/unet/00.png" alt="00.png" />
<figcaption>&sigma; = 0.0</figcaption>
</figure>
<figure>
<img src="images/unet/02.png" alt="02.png" />
<figcaption>&sigma; = 0.2</figcaption>
</figure>
<figure>
<img src="images/unet/04.png" alt="04.png" />
<figcaption>&sigma; = 0.4</figcaption>
</figure>
<figure>
<img src="images/unet/05.png" alt="05.png" />
<figcaption>&sigma; = 0.5</figcaption>
</figure>
<figure>
<img src="images/unet/06.png" alt="06.png" />
<figcaption>&sigma; = 0.6</figcaption>
</figure>
<figure>
<img src="images/unet/08.png" alt="08.png" />
<figcaption>&sigma; = 0.8</figcaption>
</figure>
<figure>
<img src="images/unet/10.png" alt="10.png" />
<figcaption>&sigma; = 1.0</figcaption>
</figure>
</div>

To start building the model, we will be using the following architecture:
<div align="center">
<figure>
<img src="images/unet/unconditioned_arch.png" alt="unconditioned_arch.png" />
<figcaption>Source: <a href="https://cal-cs180.github.io/fa25/hw/proj5/partb.html">CS180</a></figcaption>
</figure>
</div>

where <code>D</code> is the number of hidden dimensions.

<h4>Training hyperparameters</h4>
For the hyperparameters, we will be using a batch size of 256, a learning rate of <code>1e-4</code>, a hidden dimension of 128, the Adam optimizer with the given learning rate, and a training time of 5 epochs. A fixed noise level of &sigma; = 0.5 will be used to noise the training images.

<h4>Evaluation results</h4>
After the model is trained, below is the training loss curve, where the loss of the model is plotted for every batch processed:
<div align="center">
<figure>
<img src="images/unet/121_training_curve.png" alt="121_training_curve.png" />
</figure>
</div>

The following are the performance of the model after the 1st and 5th epoch on sample test images, all noised with &sigma; = 0.5:
<div align="center">
<figure>
<img src="images/unet/121_visualization.png" alt="121_visualization.png" />
</figure>
</div>

We can see that the model performs decently on different digits. To illustrate its effectiveness on images noised with different levels of &sigma; below is the model after the 5th epoch denoising the same image with different levels of noise for &sigma; &isin; [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]:
<div align="center">
<figure>
<img src="images/unet/122_visualization.png" alt="122_visualization.png" />
</figure>
</div>

Although the model works well for images with small amounts of noise, the more noise the image has, the less quality of the model's prediction.

<h4>Limitations on pure noise</h4>
Although the model is decent at removing noise from images, our goal is to generate digits from pure noise. This proves to be an issue because with MSE loss, the model will learn to predict the image that minimizes the sum of its squared distance to all other training images. To illustrate this issue, we will feed the model a pure noise sample <code>z</code> ~ &Nscr;(0, &#119816;) on all training inputs <code>x</code>, and because <code>z</code> contains no information about <code>x</code>, the result is an average of all digits in the training set.

As a result, while the training loss curve shows not much suspect:
<div align="center">
<figure>
<img src="images/unet/123_training_curve.png" alt="123_training_curve.png" />
</figure>
</div>

The following inputs and the output of the model after the 1st and 5th epoch display the average-like output of the model:
<div align="center">
<figure>
<img src="images/unet/123_visualization.png" alt="123_visualization.png" />
</figure>
</div>

To generate plausible-looking digits, we need a different approach than one-step denoising.

<h4>The Flow Matching Model</h4>
Instead of trying to denoise the image in a single step, we aim to iteratively denoise the image, similar to how we do so in the sampling loops using DeepFloyd's noise coefficients. To do this, we will start by interpolating how intermediate noise samples are constructed. The simplest approrach is to use linear interpolation, namely let the intermediate sample be <code>x<sub>t</sub></code> = (1 - <code>t</code>)x<sub>0</sub> + <code>tx<sub>1</sub></code> for a given <code>t</code> &isin; [0, 1], where <code>x<sub>0</sub></code> is the noise and <code>x<sub>1</sub></code> is the clean image.<br>

<br>Now that we have an equation relating a clean image with any pure noise sample, we can train our model to learn the <strong>flow</strong>, or the change with respect to <code>t</code> for any given <code>x<sub>t</sub></code>. This produces a vector field across all images, where the velocity for each is d/dt <code>x<sub>t</sub></code> = <code>x<sub>1</sub></code> - <code>x<sub>0</sub></code>. Therefore, if we can predict <code>x<sub>1</sub></code> - <code>x<sub>0</sub></code> for any given <code>t</code> and <code>x<sub>t</sub></code>, we can go along the path traced out by the vector field and arrived at somewhere near the manifold of clean images. This technique is known as a <strong>flow matching model</strong>, and with the model trained, we can numerically integrate a random noise sample <code>x<sub>0</sub></code> with a set number of iterations using Euler's method, and get a clean image <code>x<sub>1</sub></code>.

<h4>Training a Time-Conditioned UNet</h4>
To add time conditioning to our UNet, we will make the following changes to our model architecture:
<div align="center">
<figure>
<img src="images/unet/time_conditioned_arch.png" alt="time_conditioned_arch.png" />
<figcaption>Source: <a href="https://cal-cs180.github.io/fa25/hw/proj5/partb.html">CS180</a></figcaption>
</figure>
</div>

<h4>Flow Matching Hyperparameters</h4>
For the hyperparameters, we will be using a batch size of 64, a learning rate of <code>1e-2</code>, a hidden dimension of 64, the Adam optimizer with the given learning rate, a exponential learning rate decay scheduler with &gamma; = 0.1<sup>(1.0 / <code>num_epochs</code>)</sup>, a sampling iteration count of <code>T</code> = 50, and a training time of 10 epochs. To advance the scheduler, we will call <code>scheduler.step()</code> at the end of each training epoch.

<h4>Forward and Sampling Operations</h4>
To train our model, for each clean image <code>x<sub>1</sub></code> we will generate <code>x<sub>0</sub></code> &isin; &Nscr;(0, &#119816;) and <code>t</code> &isin; U([0, 1]), where U is the uniform distribution. After computing <code>x<sub>t</sub></code> = (1 - <code>t</code>)x<sub>0</sub> + <code>tx<sub>1</sub></code>, we will feed <code>x<sub>t</sub></code> and <code>t</code> into our UNet and compute the loss of u<sub>&theta;</sub>(<code>x<sub>t</sub></code>, <code>t</code>) and <code>x<sub>1</sub></code> - <code>x<sub>0</sub></code>. Below is the new model's training loss curve:
<div align="center">
<figure>
<img src="images/unet/21_training_curve.png" alt="21_training_curve.png" />
</figure>
</div>

When sampling from the model, we will simply generate a random <code>x<sub>0</sub></code> &isin; &Nscr;(0, &#119816;), and for every iteration <code>i</code> from 1 to <code>T</code>, we will compute <code>x<sub>0</sub></code> = <code>x<sub>0</sub></code> + (1 / <code>T</code>)u<sub>&theta;</sub>(<code>x<sub>t</sub></code>, <code>t</code>), where <code>t</code> = <code>i</code> / <code>T</code>. The following are the results ofthe 1st, 5th, and 10th epoch:
<div align="center">
<figure>
<img src="images/unet/21_visualization.png" alt="21_visualization.png" />
</figure>
</div>

Although the results are not perfect, the improvements starting from the 1st epoch up to the 10th are already noticeable.
</section>

</body>
</html>
