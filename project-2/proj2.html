<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Image Processing Report</title>
<!-- Plain HTML template (no CSS). Replace placeholder text and image sources. -->
</head>
<body>
<header>
<h1>Project 2: Filters and Frequencies</h1>
<div align="center">
<p>
by Daniel Cheng | October 1, 2025
</p>
</div>
</header>

<main>
<!-- =========================== -->
<!-- Part 1: Filters and Edges -->
<!-- =========================== -->
<section id="part1">
<h2>Part 1: Filters and Edges</h2>

<!-- 1.1 -->
<article id="part1-1">
<h3>Part 1.1: Convolutions from Scratch</h3>
<p>
Using the definition of convolution:
</p>

<pre><code>
for result_i in range(result.shape[0]):
    for result_j in range(result.shape[1]):
        total = 0
        for i in range(ffker.shape[0]):
            for j in range(ffker.shape[1]):
                total += img[result_i + i, result_j + j] * ffker[i, j]
        result[result_i, result_j] = total
return result
</code></pre>

<p>
where <code>ffker</code> is the <i>xy</i>-flipped kernel and <code>result</code> is the output 2D array, we can slightly speed up the computation by replacing the 2 inner for-loops with <code>result[result_i, result_j] = np.dot(img_flat, ffker.flatten())</code>, where <code>img_flat</code> is the flattened 1D array of the current patch of <code>img</code> based on the position of the kernel. Although this optimization produces a noticeable speed-up compared to using 4 for-loops, the fastest way is still to use <code>scipy.signal.convolve2d</code>. Below is a timing comparison of the 3 methods above using a 5x5 kernel:
</p>

<div align="center">
<img src="images/scipy.png" alt="scipy.png" width="50%">
</div>

<p>
To demonstrate an application of convolution, we can convolve an image with the <strong>box filter</strong>, a kernel with entries that sum to 1 that contains only 1 unique value. We can also convolve with the difference operators <img src="images/diff_op.png" alt="diff_op.png" width="50%"> for edge detection. Each kernel computes the difference in the <i>x</i>- or <i>y</i>-direction, so edges where the brightness of the pixel changes significantly will show up as white and black pixels on the output. Using <code>box_9x9</code> = <i>J<sub>9</sub> / 9<sup>2</sup></i> as the box filter, we get the following results:
</p>

<div align="center">
<img src="images/orgboxblur.png" alt="orgbox.png" width="50%">
</div>

<p>
For convolving with <i>D<sub>x</sub></i> and <i>D<sub>y</sub></i>, we have:
</p>

<div align="center">
<img src="images/dxdy.png" alt="dxdy.png" width="50%">
</div>

<p>
To handle boundries, we can set all of the padding pixels to 0, similar to the following:
</p>

<pre><code>
if mode == 'valid':
    return convolve(img, ker, naive)
elif mode == 'same':
    pad_ht = (ker.shape[0] - 1) // 2
    pad_wl = (ker.shape[1] - 1) // 2
    img_padded = np.zeros((img.shape[0] + ker.shape[0] - 1, img.shape[1] + ker.shape[1] - 1))
    img_padded[pad_ht:pad_ht + img.shape[0], pad_wl:pad_wl + img.shape[1]] = img
    return convolve(img_padded, ker, naive)
elif mode == 'full':
    pad_h = ker.shape[0] - 1
    pad_w = ker.shape[1] - 1
    img_fpadded = np.zeros((img.shape[0] + 2 * pad_h, img.shape[1] + 2 * pad_w))
    img_fpadded[pad_h:pad_h + img.shape[0], pad_w:pad_w + img.shape[1]] = img
    return convolve(img_fpadded, ker, naive)
else:
    raise ValueError('Unsupported mode: ' + str(mode) + '. Must be one of \'valid\', \'same\', or \'full\'.')
</code></pre>

<p>
A zero-valued pixel, however, is equivalent to a black colored pixel on the image. This means the function would introduce a dark edge if one wants to have the image be the same size after convolving, which is visible above. To prevent this scenario, the safest way is to let the first row/column be the first padding row/column on the top left side, and perform the same with the last row/column on the bottom right side. This process can also be carried out for the 2nd/2nd-to-last row/column, and so on. Compared to other approaches like <code>'wrap'</code>, this method ensures that there won't be significant artifacts by ensuring the padded pixels come from the closest pixels in the image. To implement this method, we can set the <code>boundary</code> variable to <code>'symm'</code> in <code>convolve2d</code>. In comparison, the zero-padding mode is the same as the default mode in <code>convolve2d</code>, which shows how <code>convolve2d</code> has more functionalities than the naive implementation.
</p>

</article>
<hr>

<!-- 1.2 -->
<article id="part1-2">
<h3>Part 1.2: Finite Difference Operator</h3>
<p>
Given the following image:
</p>
<div align="center">
<img src="images/cameraman.png" alt="cameraman.png" width="50%">
</div>
<p>
we can convolve this image with <i>D<sub>x</sub></i> and <i>D<sub>y</sub></i> to get the partial derivatives in the <i>x</i>- and <i>y</i>- direction. Using <code>np.dot</code>, we can then compute the gradient magnitude as follows:
</p>
<div align="center">
<img src="images/noblurnoclip.png" alt="noblurnoclip.png" width="50%">
</div>
<p>
Although the edges are visible, it's not clear, especially in the magnitude image. This is because when convolving an image with channel values in the range [0, 255], the resulting image will have a range of [-255, 255], so normalizing the image directly makes the overall image darker. To reduce the noise of the non-edge pixels, we can set a limit for each channel value before normalizing. Using a threshold of 25% from both sides, we get the following result:
</p>
<div align="center">
<img src="images/noblur.png" alt="noblur.png" width="50%">
</div>
<p>
From the image above, the edges in the gradient magnitude image are brighter and more distinct.
</p>

</article>
<hr>

<!-- 1.3 -->
<article id="part1-3">
<h3>Part 1.3: Derivative of Gaussian (DoG) Filter</h3>
<p>
To further improve edge visibility, we can first smooth out the noise by convolving the original image with a Gaussian filter. To generate one with dimensions <i>n &times; n</i>, we can take the outer product of 2 length <i>n</i> arrays. Below is the result of blurring the original image using a 5 &times; 5 Gaussian filter with &sigma; = 1:
</p>
<div align="center">
<img src="images/orggaussblur.png" alt="orggauss.png" width="50%">
</div>
<p>
Applying the same edge detection process above, we get:
</p>
<div align="center">
<img src="images/blurclip.png" alt="blurclip.png" width="50%">
</div>
<p>
To illustrate the improvements, below is a side-by-side comparison of the edge magnitudes for each of the 3 methods, in row-major order from least to most clarity:
</p>
<div align="center">
<img src="images/gradmag.png" alt="gradmag.png" width="50%">
</div>
<p>
Instead of applying 2 convolutions to the image, we can take advantage of the fact that convolution is commutative, and first convolve the Gaussian with <i>D<sub>x</sub></i> and <i>D<sub>y</sub></i>, then convolve the image with the resulting kernel. This optimizes the computation by only convolving with the image once. Below are the respective results:
</p>
<div align="center">
<img src="images/twovsone.png" alt="twovsone.png" width="50%">
</div>
</article>
</section>
<hr>

<!-- ======================== -->
<!-- Part 2: Applications -->
<!-- ======================== -->
<section id="part2">
<h2>Part 2: Applications</h2>

<!-- 2.1 -->
<article id="part2-1">
<h3>Part 2.1: Image "Sharpening"</h3>
<p>
Using convolution, we can also sharpen a blurry image with a similar technique. Since a Gaussian filter removes the highest frequencies in a signal, subtracting the filtered image from the original would leave all of the highest frequencies from the base image. We can then add this difference to the original to highlight the highest frequencies of the image, then clip it to [0, 255] to preserve brightness. Using the following Taj Mahal image:
</p>
<div align="center">
<img src="images/taj.jpg" alt="taj.jpg" width="50%">
</div>
<p>
We can obtain the blurred and high-pass filtered versions of this image as follows:
</p>
<div align="center">
<img src="images/lpvshp.png" alt="lpvshp.png" width="50%">
</div>
<p>
Now, we can add the second image to the original to get a sharpened version:
<div align="center">
<img src="images/orgsharp.png" alt="orgsharp.png" width="50%">
</div>
<p>
In general, we can change the sharpening amount by multiplying the high-pass filtered image by a constant. Below is a demonstration of various sharpening amounts from 1 to 100:
</p>
<div align="center">
<img src="onetotensquared.png" alt="onetotensquared.png" width="50%">
</div>
<p>
From the visualization above, increasing the sharpening amount highlights the high-frequency signals from the original image. Below is an example of sharpening a blurred image, using the same selfie from 1.1 as the original image, and the box-filtered version as the starting image to sharpen:
</p>
<div align="center">
<img src="images/blurthensharp.png" alt="blurthensharp.png" width="50%">
</div>
</article>
<hr>

<!-- 2.2 -->
<article id="part2-2">
<h3>Part 2.2: Hybrid Images</h3>
<p>
A <strong>hybrid image</strong> is when 2 images, one under a low-pass filter and the other a high-pass filter, are blended to create an illusion where one sees mostly the high-frequency image at a close distance, but only the low-frequency image at a longer distance. This occurs because our vision has a limited spatial frequency resolution, so higher frequencies fall outside of the frequencies visible at a sufficiently far distance. Below is an example of 2 images that we can align and create a hybrid effect:
</p>
<div align="center">
<img src="images/lowhigh.png" alt="lowhigh.png" width="50%">
</div>
<p>
To find the optimal &sigma; for each image to create the Gaussian & impulse filter with, a good starting point through experimentation is 1.8-3% times the shorter of the width and height of the LPF image, and 0.6-1.5% for the HPF image. In the example above, a cutoff percentage of 3% and 1% was used:
</p>
<div align="center">
<img src="images/lpfhpf.png" alt="lpfhpf.png" width="50%">
</div>
<p>
Below is a visualization displaying the log magnitude of the Fourier Transform of the starting and the filtered images:
</p>
<div align="center">
<img src="images/fourier.png" alt="fourier.png" width="50%">
</div>
<p>
After aligning and combining the LPF and HPF images, we get the final result below:
</p>
<div align="center">
<img src="images/hybrid1.png" alt="hybrid1.png" width="50%">
<figcaption style="margin-bottom:6px;">Cutoffs used: &sigma;<sub>1, 2</sub> = (21.96, 10.56) [732 &times; 1024, 1408 &times; 1056]</figcaption>
</div>
<p>
This effect can also be used on any 2 images that are aligned, like the following examples, which use a frequency cutoff of 2% and 0.8% times the shorter side:
</p>
<div align="center">
<img src="images/lowhigh2.png" alt="lowhigh2.png" width="50%">
<figcaption style="margin-bottom:6px;">Source: <a href="https://mobile-legends.fandom.com/wiki/Odette?file=Odette_%28Wisdom_of_the_Stars%29.jpg"><sup>[1]</sup></a> | <a href="https://mobile-legends.fandom.com/wiki/Floryn?file=Floryn_%28Melody_of_Light%29.jpg"><sup>[2]</sup></a></figcaption>
</div>
<div align="center">
<img src="images/hybrid2.png" alt="hybrid2.png" width="50%">
<figcaption style="margin-bottom:6px;">Cutoffs used: &sigma;<sub>1, 2</sub> = (7.44, 2.976) [372 &times; 372]</figcaption>
</div>
<div align="center">
<img src="images/lowhigh3.png" alt="lowhigh3.png" width="50%">
<figcaption style="margin-bottom:6px;">Source: <a href="https://mobile-legends.fandom.com/wiki/Lesley?file=Lesley_%28Angelic_Agent%29.jpg"><sup>[3]</sup></a></figcaption>
</div>
<div align="center">
<img src="images/hybrid3.png" alt="hybrid3.png" width="50%">
<figcaption style="margin-bottom:6px;">Cutoffs used: &sigma;<sub>1, 2</sub> = (9.6, 3.84) [480 &times; 720]</figcaption>
</div>
</article>
<hr>

<!-- 2.3 + 2.4 -->
<article id="part2-3">
<h3>Part 2.3 &amp; 2.4: Multiresolution Blending</h3>
<p>
To blend 2 images, we can first start by creating a Gaussian/Laplacian stack, where at
</p>
</article>
</section>
</main>
<hr>

<div align="center">
<a href="https://cjxthecoder.github.io">cjxthecoder</a> | <a href="https://github.com/cjxthecoder">GitHub</a> | <a href="https://www.linkedin.com/in/daniel-cheng-71b475279">LinkedIn</a>
</div>

</body>
</html>
