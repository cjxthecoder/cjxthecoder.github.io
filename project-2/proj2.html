<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Image Processing Report</title>
<!-- Plain HTML template (no CSS). Replace placeholder text and image sources. -->
</head>
<body>
<header>
<h1>Project 2: Fun with Filters and Frequencies</h1>
<p><strong>Author:</strong> Your Name</p>
<p><strong>Date:</strong> YYYY-MM-DD</p>
<p><strong>Course/Assignment:</strong> Course Name — Project/Report Title</p>
</header>

<main>
<!-- =========================== -->
<!-- Part 1: Filters and Edges -->
<!-- =========================== -->
<section id="part1">
<h2>Part 1: Filters and Edges</h2>

<!-- 1.1 -->
<article id="part1-1">
<h3>Part 1.1: Convolutions from Scratch!</h3>
<p>
Using the definition of convolution:
</p>

<pre><code>
for result_i in range(result.shape[0]):
    for result_j in range(result.shape[1]):
        total = 0
        for i in range(ffker.shape[0]):
            for j in range(ffker.shape[1]):
                total += img[result_i + i, result_j + j] * ffker[i, j]
        result[result_i, result_j] = total
return result
</code></pre>

<p>
where <code>ffker</code> is the <i>xy</i>-flipped kernel and <code>result</code> is the output 2D array, we can slightly speed up the computation by replacing the 2 inner for-loops with <code>result[result_i, result_j] = np.dot(img_flat, ffker.flatten())</code>, where <code>img_flat</code> is the flattened 1D array of the current patch of <code>img</code> based on the position of the kernel. Although this optimization produces a noticeable speed-up compared to using 4 for-loops, the fastest way is still to use <code>scipy.signal.convolve2d</code>. Below is a timing comparison of the 3 methods above using a 5x5 kernel:
</p>

<div align="center">
<img src="images/scipy.png" alt="scipy.png" width="50%">
</div>

<p>
To demonstrate an application of convolution, we can convolve an image with the <strong>box filter</strong>, a kernel with entries that sum to 1 that contains only 1 unique value. We can also convolve with the difference operators <img src="images/diff_op.png" alt="diff_op.png" width="50%"> for edge detection. Each kernel computes the difference in the <i>x</i>- or <i>y</i>-direction, so edges where the brightness of the pixel changes significantly will show up as white and black pixels on the output. Using <code>box_9x9</code> = <i>J<sub>9</sub> / 9<sup>2</sup></i> as the box filter, we get the following results:
</p>

<div align="center">
<img src="images/one.png" alt="one.png" width="50%">
</div>

</article>

<!-- 1.2 -->
<article id="part1-2">
<h3>Part 1.2: Finite Difference Operator</h3>
<p>
Given the following image:
</p>
<div align="center">
<img src="images/cameraman.png" alt="cameraman.png" width="50%">
</div>
<p>
we can convolve this image with <i>D<sub>x</sub></i> and <i>D<sub>y</sub></i> to get the partial derivatives in the <i>x</i>- and <i>y</i>- direction. Using <code>np.dot</code>, we can then compute the gradient magnitude as follows:
</p>
<div align="center">
<img src="images/noblurclip.png" alt="noblurclip.png" width="50%">
</div>
<p>
Although the edges are visible, it's not clear, especially in the magnitude image. This is because when convolving an image with channel values in the range [0, 255], the resulting image will have a range of [-255, 255], so normalizing the image directly makes the overall image darker. To reduce the noise of the non-edge pixels, we can set a limit for each channel value before normalizing. Using a threshold of 25% from both sides, we get the following result:
</p>
<div align="center">
<img src="images/noblur.png" alt="noblur.png" width="50%">
</div>
<p>
From the image above, the edges in the gradient magnitude image are brighter and clearer to see.
</p>

</article>

<!-- 1.3 -->
<article id="part1-3">
<h3>Part 1.3. Gaussian &amp; DoG Filters; Cameraman Comparisons</h3>
<p>
<strong>Goal:</strong> Construct Gaussian filters using <em>cv2.getGaussianKernel</em>, build Difference-of-Gaussians (DoG), visualize the filters, apply Gaussian smoothing and DoG to the cameraman image, and compare with finite difference results.
</p>
<h4>Filter Visualizations</h4>
<figure>
<img src="images/gaussian_filter_viz.png" alt="Gaussian filter visualization" />
<figcaption>Gaussian filter visualization.</figcaption>
</figure>
<figure>
<img src="images/dog_filter_viz.png" alt="DoG filter visualization" />
<figcaption>Difference-of-Gaussians (DoG) visualization.</figcaption>
</figure>
<h4>Applications to Cameraman</h4>
<figure>
<img src="images/cameraman_gaussian.png" alt="Cameraman after Gaussian smoothing" />
<figcaption>Cameraman after Gaussian smoothing.</figcaption>
</figure>
<figure>
<img src="images/cameraman_dog.png" alt="Cameraman after DoG filtering" />
<figcaption>Cameraman after DoG filtering.</figcaption>
</figure>
<figure>
<img src="images/cameraman_finite_diff.png" alt="Cameraman finite difference results" />
<figcaption>Finite difference method results for comparison.</figcaption>
</figure>
<p><strong>Comparison &amp; Discussion:</strong> [Analyze differences in edge localization, noise sensitivity, and parameter effects.]</p>
</article>
</section>

<!-- ======================== -->
<!-- Part 2: Applications -->
<!-- ======================== -->
<section id="part2">
<h2>Part 2: Applications</h2>

<!-- 2.1 -->
<article id="part2-1">
<h3>Part 2.1. Unsharp Mask</h3>
<p>
<strong>Goal:</strong> Implement unsharp masking. Explain how it relates to blur filters and high-frequency components. Show blurred, high-frequency, and sharpened versions of the Taj Mahal image and another image of your choice. Demonstrate varying the sharpening amount.
</p>
<h4>Taj Mahal</h4>
<figure>
<img src="images/taj_blurred.png" alt="Taj Mahal blurred" />
<figcaption>Blurred version (Taj Mahal).</figcaption>
</figure>
<figure>
<img src="images/taj_highfreq.png" alt="Taj Mahal high-frequency component" />
<figcaption>High-frequency component (Taj Mahal).</figcaption>
</figure>
<figure>
<img src="images/taj_sharpened_low.png" alt="Taj Mahal sharpened (low amount)" />
<figcaption>Sharpened result (low amount).</figcaption>
</figure>
<figure>
<img src="images/taj_sharpened_med.png" alt="Taj Mahal sharpened (medium amount)" />
<figcaption>Sharpened result (medium amount).</figcaption>
</figure>
<figure>
<img src="images/taj_sharpened_high.png" alt="Taj Mahal sharpened (high amount)" />
<figcaption>Sharpened result (high amount).</figcaption>
</figure>
<h4>Additional Image</h4>
<figure>
<img src="images/other_blurred.png" alt="Additional image blurred" />
<figcaption>Blurred version (additional image).</figcaption>
</figure>
<figure>
<img src="images/other_highfreq.png" alt="Additional image high-frequency component" />
<figcaption>High-frequency component (additional image).</figcaption>
</figure>
<figure>
<img src="images/other_sharpened.png" alt="Additional image sharpened" />
<figcaption>Sharpened result (additional image).</figcaption>
</figure>
<p><strong>Explanation:</strong> [Summarize the unsharp mask formula, role of blur radius/sigma, and artifact considerations.]</p>
</article>

<!-- 2.2 -->
<article id="part2-2">
<h3>Part 2.2. Hybrid Images</h3>
<p>
<strong>Goal:</strong> Create three hybrid images (including Derek + Nutmeg and two others). For one hybrid, show the full pipeline; for the others, show originals and final hybrid only.
</p>
<h4>Full Pipeline Hybrid (Detailed)</h4>
<figure>
<img src="images/hybrid1_originalA.png" alt="Original A (aligned)" />
<figcaption>Original A (aligned).</figcaption>
</figure>
<figure>
<img src="images/hybrid1_originalB.png" alt="Original B (aligned)" />
<figcaption>Original B (aligned).</figcaption>
</figure>
<figure>
<img src="images/hybrid1_fftA.png" alt="Fourier transform of A" />
<figcaption>Fourier transform (A).</figcaption>
</figure>
<figure>
<img src="images/hybrid1_fftB.png" alt="Fourier transform of B" />
<figcaption>Fourier transform (B).</figcaption>
</figure>
<figure>
<img src="images/hybrid1_filteredA.png" alt="Filtered A" />
<figcaption>Filtered result (A).</figcaption>
</figure>
<figure>
<img src="images/hybrid1_filteredB.png" alt="Filtered B" />
<figcaption>Filtered result (B).</figcaption>
</figure>
<figure>
<img src="images/hybrid1_cutoff.png" alt="Cutoff frequency visualization/justification" />
<figcaption>Cutoff frequency choice and justification.</figcaption>
</figure>
<figure>
<img src="images/hybrid1_final.png" alt="Final hybrid image" />
<figcaption>Final hybrid image (detailed pipeline).</figcaption>
</figure>

<h4>Additional Hybrids (Brief)</h4>
<figure>
<img src="images/hybrid2_originals.png" alt="Originals for Hybrid 2" />
<figcaption>Original images for Hybrid 2.</figcaption>
</figure>
<figure>
<img src="images/hybrid2_final.png" alt="Hybrid 2 final" />
<figcaption>Hybrid 2 — final result.</figcaption>
</figure>
<figure>
<img src="images/hybrid3_originals.png" alt="Originals for Hybrid 3" />
<figcaption>Original images for Hybrid 3.</figcaption>
</figure>
<figure>
<img src="images/hybrid3_final.png" alt="Hybrid 3 final" />
<figcaption>Hybrid 3 — final result.</figcaption>
</figure>
</article>

<!-- 2.3 + 2.4 -->
<article id="part2-3">
<h3>Part 2.3 &amp; 2.4. Gaussian/Laplacian Stacks; Figure 3.42(a–l); Custom Blends</h3>
<p>
<strong>Goal:</strong> Visualize Gaussian and Laplacian stacks for the Orange + Apple images, recreate outcomes similar to Figure 3.42(a)–(l), and include two custom blends (one with an irregular mask).
</p>
<h4>Gaussian &amp; Laplacian Stacks (Orange + Apple)</h4>
<figure>
<img src="images/orapple_gaussian_stack.png" alt="Gaussian stack visualization" />
<figcaption>Gaussian stack visualization (Orange + Apple).</figcaption>
</figure>
<figure>
<img src="images/orapple_laplacian_stack.png" alt="Laplacian stack visualization" />
<figcaption>Laplacian stack visualization (Orange + Apple).</figcaption>
</figure>
<h4>Recreated Figure 3.42(a–l)</h4>
<figure>
<img src="images/figure342_grid.png" alt="Grid of results analogous to Fig. 3.42(a–l)" />
<figcaption>Outcomes analogous to Figure 3.42(a–l).</figcaption>
</figure>
<h4>Custom Blended Images</h4>
<figure>
<img src="images/custom_blend1.png" alt="Custom blended image with irregular mask" />
<figcaption>Custom blend #1 (irregular mask).</figcaption>
</figure>
<figure>
<img src="images/custom_blend2.png" alt="Custom blended image (straight mask)" />
<figcaption>Custom blend #2.</figcaption>
</figure>
<p><strong>Notes &amp; Discussion:</strong> [Describe masks, blending levels, feathering choices, and artifacts.]</p>
</article>
</section>
</main>

<footer>
<p><strong>References:</strong> [List any papers, libraries, and datasets used.]</p>
</footer>
</body>
</html>
