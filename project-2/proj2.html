<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Image Processing Report</title>
<!-- Plain HTML template (no CSS). Replace placeholder text and image sources. -->
</head>
<body>
<header>
<h1>Project 2: Fun with Filters and Frequencies</h1>
<div align="center">
<p>
by Daniel Cheng | September 29, 2025
</p>
</div>
</header>

<main>
<!-- =========================== -->
<!-- Part 1: Filters and Edges -->
<!-- =========================== -->
<section id="part1">
<h2>Part 1: Filters and Edges</h2>

<!-- 1.1 -->
<article id="part1-1">
<h3>Part 1.1: Convolutions from Scratch!</h3>
<p>
Using the definition of convolution:
</p>

<pre><code>
for result_i in range(result.shape[0]):
    for result_j in range(result.shape[1]):
        total = 0
        for i in range(ffker.shape[0]):
            for j in range(ffker.shape[1]):
                total += img[result_i + i, result_j + j] * ffker[i, j]
        result[result_i, result_j] = total
return result
</code></pre>

<p>
where <code>ffker</code> is the <i>xy</i>-flipped kernel and <code>result</code> is the output 2D array, we can slightly speed up the computation by replacing the 2 inner for-loops with <code>result[result_i, result_j] = np.dot(img_flat, ffker.flatten())</code>, where <code>img_flat</code> is the flattened 1D array of the current patch of <code>img</code> based on the position of the kernel. Although this optimization produces a noticeable speed-up compared to using 4 for-loops, the fastest way is still to use <code>scipy.signal.convolve2d</code>. Below is a timing comparison of the 3 methods above using a 5x5 kernel:
</p>

<div align="center">
<img src="images/scipy.png" alt="scipy.png" width="50%">
</div>

<p>
To demonstrate an application of convolution, we can convolve an image with the <strong>box filter</strong>, a kernel with entries that sum to 1 that contains only 1 unique value. We can also convolve with the difference operators <img src="images/diff_op.png" alt="diff_op.png" width="50%"> for edge detection. Each kernel computes the difference in the <i>x</i>- or <i>y</i>-direction, so edges where the brightness of the pixel changes significantly will show up as white and black pixels on the output. Using <code>box_9x9</code> = <i>J<sub>9</sub> / 9<sup>2</sup></i> as the box filter, we get the following results:
</p>

<div align="center">
<img src="images/orgbox.png" alt="orgbox.png" width="50%">
</div>

<p>
For convolving with <i>D<sub>x</sub></i> and <i>D<sub>y</sub></i>, we have:
</p>

<div align="center">
<img src="images/dxdy.png" alt="dxdy.png" width="50%">
</div>

</article>
<hr>

<!-- 1.2 -->
<article id="part1-2">
<h3>Part 1.2: Finite Difference Operator</h3>
<p>
Given the following image:
</p>
<div align="center">
<img src="images/cameraman.png" alt="cameraman.png" width="50%">
</div>
<p>
we can convolve this image with <i>D<sub>x</sub></i> and <i>D<sub>y</sub></i> to get the partial derivatives in the <i>x</i>- and <i>y</i>- direction. Using <code>np.dot</code>, we can then compute the gradient magnitude as follows:
</p>
<div align="center">
<img src="images/noblurnoclip.png" alt="noblurnoclip.png" width="50%">
</div>
<p>
Although the edges are visible, it's not clear, especially in the magnitude image. This is because when convolving an image with channel values in the range [0, 255], the resulting image will have a range of [-255, 255], so normalizing the image directly makes the overall image darker. To reduce the noise of the non-edge pixels, we can set a limit for each channel value before normalizing. Using a threshold of 25% from both sides, we get the following result:
</p>
<div align="center">
<img src="images/noblur.png" alt="noblur.png" width="50%">
</div>
<p>
From the image above, the edges in the gradient magnitude image are brighter and more distinct.
</p>

</article>
<hr>

<!-- 1.3 -->
<article id="part1-3">
<h3>Part 1.3: Derivative of Gaussian (DoG) Filter</h3>
<p>
To further improve edge visibility, we can first smooth out the noise by convolving the original image with a Gaussian filter. To generate one with dimensions <i>n &times; n</i>, we can take the outer product of 2 length <i>n</i> arrays. Below is the result of blurring the original image using a 5 &times; 5 Gaussian filter with &sigma; = 1:
</p>
<div align="center">
<img src="images/orggauss.png" alt="orggauss.png" width="50%">
</div>
<p>
Applying the same edge detection process above, we get:
</p>
<div align="center">
<img src="images/blurclip.png" alt="blurclip.png" width="50%">
</div>
<p>
To illustrate the improvements, below is a side-by-side comparison of the edge magnitudes for each of the 3 methods, in row-major order from least to most clarity:
</p>
<div align="center">
<img src="images/gradmag.png" alt="gradmag.png" width="50%">
</div>
<p>
Instead of applying 2 convolutions to the image, we can take advantage of the fact that convolution is commutative, and first convolve the Gaussian with <i>D<sub>x</sub></i> and <i>D<sub>y</sub></i>, then convolve the image with the resulting kernel. This optimizes the computation by only convolving with the image once. Below are the respective results:
</p>
<div align="center">
<img src="images/twovsone.png" alt="twovsone.png" width="50%">
</div>
</article>
</section>
<hr>

<!-- ======================== -->
<!-- Part 2: Applications -->
<!-- ======================== -->
<section id="part2">
<h2>Part 2: Applications</h2>

<!-- 2.1 -->
<article id="part2-1">
<h3>Part 2.1: Image "Sharpening"</h3>
<p>
Using convolution, we can also sharpen a blurry image with a similar technique. Since a Gaussian filter removes the highest frequencies in a signal, subtracting the filtered image from the original would leave all of the highest frequencies from the base image. We can then add this difference to the original to highlight the highest frequencies of the image, then clip it to [0, 255] to preserve brightness. Using the following Taj Mahal image:
</p>
<div align="center">
<img src="images/taj.jpg" alt="taj.jpg" width="50%">
</div>
<p>
We can obtain the blurred and high-pass filtered versions of this image as follows:
</p>
<div align="center">
<img src="images/lpvshp.png" alt="lpvshp.png" width="50%">
</div>
<p>
Now, we can add the second image to the original to get a sharpened version:
<div align="center">
<img src="images/orgsharp.png" alt="orgsharp.png" width="50%">
</div>
<p>
In general, we can change the sharpening amount by multiplying the high-pass filtered image by a constant. Below is a demonstration of various sharpening amounts from 1 to 100:
</p>
<div align="center">
<img src="onetotensquared.png" alt="onetotensquared.png" width="50%">
</div>
<p>
From the visualization above, increasing the sharpening amount highlights the high-frequency signals from the original image. Below is an example of sharpening a blurred image, using the same selfie from 1.1 as the original image, and the box-filtered version as the starting image to sharpen:
</p>
<div align="center">
<img src="images/blurthensharp.png" alt="blurthensharp.png" width="50%">
</div>
</article>
<hr>

<!-- 2.2 -->
<article id="part2-2">
<h3>Part 2.2: Hybrid Images</h3>
<p>
A <strong>hybrid image</strong> is when 2 images, one under a low-pass filter and the other a high-pass filter, are blended to create an illusion where one sees mostly the high-frequency image at a close distance, but only the low-frequency image at a longer distance. This occurs because our vision has a limited spatial frequency resolution, so higher frequencies fall outside of the frequencies visible at a sufficiently far distance. Below is an example of 2 images that we can align and create a hybrid effect:
</p>
<div align="center">
<img src="images/lowhigh.png" alt="lowhigh.png" width="50%">
</div>
To find the optimal &sigma; for each image to create the Gaussian & impulse filter with, a good starting point
<!-- 2.3 + 2.4 -->
<article id="part2-3">
<h3>Part 2.3 &amp; 2.4. Gaussian/Laplacian Stacks; Figure 3.42(a–l); Custom Blends</h3>
<p>
<strong>Goal:</strong> Visualize Gaussian and Laplacian stacks for the Orange + Apple images, recreate outcomes similar to Figure 3.42(a)–(l), and include two custom blends (one with an irregular mask).
</p>
<h4>Gaussian &amp; Laplacian Stacks (Orange + Apple)</h4>
<figure>
<img src="images/orapple_gaussian_stack.png" alt="Gaussian stack visualization" />
<figcaption>Gaussian stack visualization (Orange + Apple).</figcaption>
</figure>
<figure>
<img src="images/orapple_laplacian_stack.png" alt="Laplacian stack visualization" />
<figcaption>Laplacian stack visualization (Orange + Apple).</figcaption>
</figure>
<h4>Recreated Figure 3.42(a–l)</h4>
<figure>
<img src="images/figure342_grid.png" alt="Grid of results analogous to Fig. 3.42(a–l)" />
<figcaption>Outcomes analogous to Figure 3.42(a–l).</figcaption>
</figure>
<h4>Custom Blended Images</h4>
<figure>
<img src="images/custom_blend1.png" alt="Custom blended image with irregular mask" />
<figcaption>Custom blend #1 (irregular mask).</figcaption>
</figure>
<figure>
<img src="images/custom_blend2.png" alt="Custom blended image (straight mask)" />
<figcaption>Custom blend #2.</figcaption>
</figure>
<p><strong>Notes &amp; Discussion:</strong> [Describe masks, blending levels, feathering choices, and artifacts.]</p>
</article>
</section>
</main>

<footer>
<p><strong>References:</strong> [List any papers, libraries, and datasets used.]</p>
</footer>
</body>
</html>
